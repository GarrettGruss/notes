## Reliability Models

## Basic Functions and Definitions

- ğ¹(ğ‘¡): cumulative distribution function(CDF) for failure over time
  * 
- f(ğ‘¡): Probability density function (PDF), f(ğ‘¡) = ğ¹â€²(ğ‘¡)
- Reliability function ğ‘…(ğ‘¡)=1Â âˆ’ğ¹(ğ‘¡)
  * ğ‘…(ğ‘¡)=ğ‘ƒ(ğ‘‡â‰¥ğ‘¡)=ğ‘ƒ(ğ‘›ğ‘œÂ ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’Â ğ‘ğ‘¦Â ğ‘¡)
- Hazard function/rate/intensity : ğ“(ğ‘¡)â–³ğ‘¡=ğ‘ƒ{Â ğ‘¡Â <ğ‘‡<ğ‘¡+Â â–³ğ‘¡|ğ‘‡>ğ‘¡}
- Mean function m(ğ‘¡) in Non-homogeneous Poisson Process (NHPP)
- Failure rate/intensity,  ğœ†(ğ‘¡)=Â ğ‘šâ€²(ğ‘¡)
- Time domain definition: Failure in timespace
  * ğ‘…=ğ‘ /ğ‘›=(ğ‘›âˆ’ğ‘“)/ğ‘›=1âˆ’ğ‘“/ğ‘›=1âˆ’ğ‘Ÿ
  * In this reliability model, s and N (shown as lowercase n in your equation) represent:

s = number of successful operations or non-failed components
n = total number of operations or total number of components

From the equation you've provided:
R = s/n = (n - f)/n = 1 - f/n = 1 - r
Where:

R = reliability (probability of success)
s = number of successes = (n - f)
n = total number of trials/components
f = number of failures
r = failure rate or failure ratio (f/n)

- Mean time between failures (MTBF), Mean time to failure (MTTF), etc.

---

## SRGM Classification

### Data used

**Time-between-failure (TBF) models**
- Random variable: Failure interval

**Failure-count (FC) models**
- Random variable: failure count for given interval
- Most widely used (in this class)

Some models can use both TBF and FC data

### Other classifications possible
- **Time measurement:**
  - Calendar/wall-clock/execution/etc. time
- **F-Distribution**
  - Poisson â€“ fixed rate interval
  - Binomial â€“ two possible outcomes etc.
- Finite vs infinite failures
- Tools: pre-selected models

---

## TBF Models

You are trying to look at failure intervals over time. 

Model characteristics 
Failure intervals as random variable
ğ‘‡_ğ’¾: random variable for the time between (ğ’¾âˆ’1)^ğ‘ ğ‘¡ and ğ’¾^ğ‘¡h failures 
Distribution/density: ğ¹_ğ’¾(ğ‘¡) or ğ‘“_ğ’¾(ğ‘¡) 
Directly define ğ“_ğ’¾(ğ‘¡)  
Relate ğ“_ğ’¾(ğ‘¡) to failures/faults 
Defining TBF models 
Sequence of ğ“_ğ’¾(ğ‘¡) over ğ’¾
Initial value? 
Physical interpretation possible? 
Rate (or cumulative) data plotting

---

## TBF1: Jelinski-Moranda
One of the earliest model using TBF (time-between-failure) measurement 
Developed in 1972
Failure rate (ğ“_ğ’¾Â ğ‘œğ‘ŸÂ ğœ†_ğ’¾): 
Proportional to defects remaining 
Step function: ğ“_ğ’¾=Â ğœ™Â (Â ğ‘Â âˆ’(ğ’¾âˆ’1))
ğ“_ğ’¾Â : failure rate for the ğ’¾^ğ‘¡h failure 
Two model parameters:
 Ï† constant for failure exposure
 N constant for total defects 
Plotting ğ“_ğ’¾â€²s and reliability growth 
Relation to later models 
Similar assumptions 
Other failure rate: geometric etc. 
Continuous version: Goel-Okumoto etc.

These models are looking at now to the past, so at this point we already know the total defects in the system. So when you go back to the beginning, it is a long time looking at it. You always consider looking at the final defects, and you want to track how the defects grow to the final cumulative number. YOu will be adjusting the model to best fit your distribution of failures.

Some of these are generic, but others are logarithmic.
---

## TBF2-3: Schick-Wolverton
Variations of Jelinski-Moranda (TBF1) model
Schick-Wolverton linear model (TBF2): 
Proportional to defects remaining and time 
Slope function with renewal 
 ğœ†_ğ’¾=Â ğœ™Â (Â ğ‘Â âˆ’(ğ‘–Â âˆ’1))ğ‘¡
Assumptions/parameters like TBF1 
Schick-Wolverton parabolic model (TBF3): 
2^ğ‘›ğ‘‘ order (parabolic) time renewal 
ğœ†_ğ’¾=Â ğœ™Â (Â ğ‘Â âˆ’(ğ’¾âˆ’1))(ğ‘ğ‘¡^2+ğ‘ğ‘¡+ğ‘)
Assumptions/parameters like TBF2 
Plotting ğœ†_ğ’¾â€²s and reliability growth 

They took the jelinski model and they modified it. THe proportion of defects remaining. The paremters are pretty much the same. Depending on your failure distribution, you can fit a parameter in and create a new model. The idea is you want to cosntruct the model. The goal of these models is to construct models that mirror historical recorded defects on a project, so you can reuse these models on future projects to predict the state of their defects.

The goal of these models is to reuse historical trends on future software projects to project defects based on similar software projects.
---

## TBF4: Geometric Models (Moranda)
Just like Jelinski-Moranda (TBF1) 
Failure rate 
Step function but geometric step sizes 
ğœ†_ğ’¾=ğœ†_0Â ğœ™^(ğ’¾âˆ’1)
ğœ†_ğ’¾Â : failure rate for the ğ’¾^ğ‘¡h failure 
Two model parameters:
â€“ Ï†: Step reduction/curvature
â€“ ğœ†_0Â : Initial failure rate 
Plotting and comparison to TBF1 
Relation to later models 
Close relation to Musa-Okumoto model (logarithmic Poisson) 
Models defect discovery situations 
Hybrid geometric Poisson 
ğœ†_ğ‘–=ğœ†_0Â ğœ™^(ğ’¾Â âˆ’1)Â +Â ğ‘

---

## TBF5: Imperfect Debugging

Goel-Okumoto 
Failure rate 
Just like Jelinski-Moranda (JM)
Step function 
Allow for imperfect debugging 
ğœ†_ğ’¾=Â ğœ™Â (Â ğ‘Â âˆ’ğ“…(ğ‘–Â âˆ’1))
ğ“…: prob(imperfect debugging) 
Other parameters same 
Parameter re-interpretation as JM 
Relation to later models 
Close relation to Goel-Okumoto NHPP model 
Models defect removal process

This model alows "imperfect debugging". YOu keep adding parameters into the model to simulare imperfect debugging. The original models did not look at the initial failures, and as the models mature we introduce more parameters. This model incorporates the initial failure rate. The step reduction/curvature is how fast you reduce your defects.

---

## TBF6: Littlewood-Verrall (LV)

Bayesian model 
 ğ‘¡_ğ’¾Â : ğ’¾^ğ‘¡hÂ inter-failure interval 
Distribution (Probability density function) for ğ‘¡_ğ’¾:
ğ‘“(ğ‘¡_ğ’¾âˆ£ğœ†_ğ’¾)=ğœ†_ğ’¾â„¯^(âˆ’ğœ†_ğ’¾ ğ‘¡_ğ’¾)
 ğœ†_ğ’¾Â : Failure rate parameter 
Distribution (PDF) for ğœ†_ğ’¾Â :
ğ‘“(ğ‘¡_ğ’¾|ğ›¼,ğœ“(ğ’¾))=([ğœ“(ğ’¾)]^ğ›¼ (ğœ†_ğ’¾)^(ğ›¼âˆ’1) ğ”¢^(âˆ’ğœ“(ğ’¾)ğœ†_ğ’¾))/âŒˆ(ğ›¼)â”¤
ğœ“(ğ‘–) : Increasing function of ğ’¾Â  
ğ›¼Â : Constant 
In SMERFS, LV model with ğœ“(ğ‘–): 
ğœ“(ğ‘–)=Â ğ›½_0+ğ›½_1ğ’¾, or 
ğœ“(ğ‘–)=Â ğ›½_0+ğ›½_1ğ’¾^2
---

## Failure-count (FC) Models

Model characteristics 
Failure count ğ‘_ğ’¾Â random variable 
Time interval: predefined
Equal: Schneidewind model
Different: other models 
Distribution: failure arrival process 
Directly define process parameters 
NHPP most common 
Defining FC models 
Time intervals 
Underlying stochastic processes 
Physical interpretation 
Cumulative (or rate) data plotting

---

## FC1: Goel-Okumoto

Process assumption: NHPP (Non-homogeneous Poisson Process) 
Model definition: 
Probability of ğ‘› failures in [0,ğ‘¡]:
ğ‘ƒ(ğ‘(ğ‘¡)=ğ‘›)=Â (ğ‘š(ğ‘¡)^ğ‘›)/ğ‘›!ğ‘’^(âˆ’ğ‘š(ğ‘¡))
ğ‘š(ğ‘¡)Â : Mean function 
ğ‘š(ğ‘¡)=ğ‘(1âˆ’ğ‘’^(âˆ’ğ‘ğ‘¡))
ğœ†(ğ‘¡)=ğ‘š^â€²(ğ‘¡): failure rate 
ğœ†(ğ‘¡)=Â ğ‘ğ‘ğ‘’^(âˆ’ğ‘ğ‘¡)
ğ‘: Total estimated failures 
ğ‘: Failure exposure as model curvature 
Data: Period failure count (PFC model) 
(N(ğ‘¡)Â is the random variable)

---

## FC Models: Other NHPP

---

## FC Models: Generalized Poisson

---

## FC Models: Brooks-Motley

---

## FC Models: Musa

---

## FC Models: Musa (Continued)

### Practicality of Musa models
- Software usage; operational profile and execution time
- Predictions (prescriptive) based on process and product characteristics
- Practical issues dealt in Musa book
- Practicality verses theoretical focus

### Applications of Musa models
- AT&T projects: 10-20%
- Best practice at AT&T (Lyu/HSRE Ch.6)
- Adoption in other environments

### Tool and other support
- AT&T's SRE Toolkit
- Training and benchmarking
- Most publicized success stories

---

## Choice of SRGMs

### Issues discussed before
- Goal/environment/experience
- Tool/data availability

### Other model choice issues
- Time measurement and model fit
- Single vs. multiple models
- Composite models possible/meaningful?
- Existing vs. new models
- Assumptions/limitations/applicability

---

## Choice of SRGMs (Continued)

### Time measurement and model fit
- Experience at AT&T (exec. time!)
- IBM experience
- Bad fit implies time appropriate?
- Compare to bad fit implies another model

### Single vs. multiple models
- Best fitted vs. optimistic (fast reliability growth) vs. pessimistic (slow reliability growth)
- Band/range instead of single estimate
- Related: Synthesized/Composite models

### Existing vs. new models
- Simplicity of existing models
- Validation of new models
- Caution against ad-hoc new models

---

## Alternatives to SRGMs

### Reliability: Prob(failure-free operation)
- **Time:** How to measure â‡’ SRGMs
- **Input:** Characterize/classify
- Assumptions: Failure/OP/time/distribution
- Applicability and limitations

### Alternatives to SRGMs
- Input domain/combinatorial
- Fault seeding
- Hybrid models: Cleanroom model
- Coverage-based and predictive
- TBRMs: tree-based reliability models
  - Both time/input information

---

## Nelson's Input Domain Model

Nelson Model: 
Running for a sample of n inputs. 
Randomly selected from set
ğ¸={ğ¸_ğ’¾ :ğ’¾=1,2,â€¦,ğ‘}
Sampling probability vector: 
{ğ‘ƒ_ğ’¾ :ğ’¾=1,2,â€¦,ğ‘}
{ğ‘ƒ_ğ’¾}: Operational profile. 
Number of failures: ğ‘“. 
Estimated reliability = success rate
ğ‘…=(ğ‘›âˆ’ğ‘“)/ğ‘›=1âˆ’ğ‘“/ğ‘›=1âˆ’ğ‘Ÿ
ğ‘Ÿ: failure rate. 
Repeated sampling without fixing.

# Nelson's Input Model

Nelson's input model is a **statistical software reliability testing approach** that estimates reliability by sampling test cases according to their operational usage patterns.

## Core Concept

Instead of testing all possible inputs exhaustively or uniformly, you test a **random sample of n inputs** drawn from the operational profile â€” a probability distribution that reflects how the software will actually be used in practice.

## Key Components

**Operational Profile (Ï€)**: A probability vector where each Ï€_i represents the likelihood that input i will be used in real operation. Inputs used more frequently get higher sampling probabilities.

**Sampling Process**: You randomly select n test inputs according to Ï€, run them through the software, and observe failures.

**Reliability Estimation**: After observing f failures out of n tests, the estimated reliability is simply:
- RÌ‚ = (n - f) / n = success rate

The estimated failure rate Î¸Ì‚ is the complement: Î¸Ì‚ = f/n.

## Important Feature

Nelson's model uses **sampling without replacement of fixes** â€” meaning you continue testing with the faulty software, recording failures as they occur, without stopping to fix bugs between test runs. This provides a snapshot of current reliability under operational conditions.

## Purpose

This model bridges the gap between testing and real-world usage. By weighting tests toward common use cases, it provides a realistic estimate of the reliability users will actually experience, rather than treating all inputs as equally likely.

---

## Other Input Domain Models

Brown-Lipow model: 
Explicit input state distribution. 
Known probability for sub-domains ğ¸_ğ’¾  

ğ‘“_ğ’¾ failures for ğ‘›_ğ’¾ runs from sub-domain ğ¸_ğ’¾ 
ğ‘…=1âˆ’âˆ‘_(ğ’¾=1)^ğ‘â–’ã€–(ğ‘“_ğ’¾)/(ğ‘›_ğ’¾)ã€—ğ‘ƒ(ğ¸_ğ’¾)
Ramamoorthy-Bastani: 
Safety critical systems, ğ‘…Ì‚=1
Confidence level for ğ‘…Ì‚Â 
ğ“_ğ’¾ specific set of inputs 
P(program correct | correct for ğ“_ğ’¾â€™s) 
ğ‘ƒ=ğ‘’^(âˆ’ğœ†ğ‘‰)âˆ_(ğ’¾=1)^(ğ‘›âˆ’1)â–’ã€–2/(1+ğ‘’^(âˆ’ğœ† ğ“_ğ’¾))ã€—
ğœ† source code complexity 
Recent development by Woit-Parnas


---

## Ho's Input Domain Model

---

## Mills Fault Seeding Model

---

## Cleanroom Reliability Model

---

## Coverage and Coverage-Based Models

### Alternative: coverage analysis
- Defect fixing effect
- Infeasibility of exhaustive testing

### Pure coverage vs. coverage-based models

### Focus on input/internal state coverage
- Function/data/statement coverage
- Path and dependency coverage
- **Assumption:** When coverage increases it implies reliability increases
  - A qualitative relation and not quantified

### Coverage-based modeling
- Analytical: Weyuker etc.
- Empirical: Mathur etc.
- Mixed: Chen/Lyu/Wong

---

## AI/ML-Based Models

### Alternative: AI/ML-Based Models
- AI/ML linkage to modeling techniques
  - Especially to risk identification/analysis
- Existence of mostly massive data
- Mining software repositories

### Characteristics and limitations
- Focus on numbers/quantities instead of functional forms
- Often deal with defects directly instead of reliability
- Possible integration with existing Software Reliability Models?
- TBRM might represent this direction?

---

## General Assumptions and Implications

### Times between failures are independent
- Implies randomized testing
- **Practical scenarios:**
  - Defect fixing effect
  - Structure/progression in testing

### Immediate defect removal
- Duplicate defect counting
- Related but not duplicate?
- Infeasible for in-field defects
- If similar defects are tracked as seperate items, defects can be duplicated. There is a high probability of defect duplications.

### No new fault injected
- Reliability growth assured
- **Practical:** Injection normally less than the removal
- Related: Decreasing failure rate

---

## Assumptions and Implications (Continued)

### Relating failure rate to number of faults
- **Variations to the assumption**
  - Proportionality between the two
  - Functional relation between the two
  - Time dependent relation

### Implications of failure detection and detection sequences

### Operational profile
- Ensures reasonable/meaningful reliability assessments and predictions
- Limits applicability

### Time as a basis for failure rate
- Equivalent time units
- Requires proper time measurement

---

## Assumptions and Applicability

### General considerations
- Assumptions for different model types
- Match them to application environment
- Models necessarily simple
- Impossible perfect match

### Applicability to different processes
- Waterfall generally assumed
- **Testing phases**
  - Used Based Statistical Testing
  - Including Black Box Testing: SRGMs and Input Domain Models
  - White Box Testing: coverage
- Incremental development: Cleanroom
- Spiral model: Iterations
- **Operational phases**
  - Difference in defect removal
  - Data availability

---

## Applicability to Different Phases

A strong usage of the reliability growth model is to manage your customer's expectations.

### Requirement and specification
- Reliability goal from customer expectation
- Feasibility and affordability
- Operational profile construction
- Prepare for random testing

### Design and coding
- Fault detection and removal (QA)
- Musa's prescriptive model
- Other existing models not applicable
- **Alternative models may be needed**
  - Fault and error-based models
  - Constructive information (white box)
  - Predictive models relating to reliability

---

## Applicability to Different Phases (Continued)



### Unit testing
- White-box deterministic testing
- Tester = developer
- Applicable: fault seeding, coverage-based
- Other models not applicable

### Integration and system testing
- Functional/system Verification Testing, regression, integration
- Focus: Customer-oriented operations
- Less emphasis on coverage
- Main phase for SRGMs
- Failure Count models more robust
- Random testing conformance
- Use of other models

---

## Applicability to Different Phases (Further)

### Acceptance testing
- Gate: accept/release or not
- Also plan for product support
- Basis: snapshot(s) or random sampling
- Cleanroom-like model usage
- Input domain model also appropriate

### Operational phase
- Actual operations (post-release)
- Beta or pre-release
- Difference in operational environments
- Data availability and treatment
- Reliability vs. availability
- Defect fix and product refreshing
- Business decisions

---

## Applications and Examples

### Overall procedure
- A lot of preparation
- Generic: preparation/modeling/follow-up
- Routine procedure once started
- Often periodic activities
- Evaluation/feedback/improvement

### Application examples
- Data: telecommunications (Musa)
- Wide applications of Goel-Okumoto, Musa, and other models
- Shuttle: Schneidewind and Keller
- Examples in IBM

---

## Recent Applications and Examples

### New application examples
- Non-traditional (beyond commercial and telecommunication industries)
- From product to service (Lyu etc.)
- Combined with traditional statistics (Pham etc.)
- Link to metrics/risk (Khoshgoftaar etc.)

### New applications
- SMU work
- Web sites and applications (Abuta-Tian)
- Defense
- E-commerce
- Service and cloud (including APIs)
- Open source & public data/forums
- Combination with other quality concerns: safety and usability

---

## Hazard and Risk Resolution

### Components
- Hazard Resolution and Damage Control
- **Hazard Resolution Techniques:**
  - Hazard Elimination/Reduction/Control
- **Risk Resolution:**
  - Damage Reduction

---

## Safety Techniques

### Hazard and risk identification
- Accident scenarios: actual/hypothetical
- Starting points for safety
- Focus: operations and operational env.

### Hazard analysis and assessment
- Fault trees: (static) logical conditions
- Event trees: dynamic sequences
- Other analyses/assessment techniques

### Hazard and risk resolution
- Hazard elimination
- Hazard reduction
- Hazard control âŠ² Damage control

---

## Hazard and Risk Resolution

### Generic hazard resolution techniques (in order of their precedence)

1. **Hazard elimination:**
   - Eliminate (some) hazard sources

2. **Hazard reduction:**
   - Reduce hazard likelihood/severity

3. **Hazard control:**
   - Control hazard severity/scope

**Hazard resolution implies the probability of an accident reduces**

### Related issues
- Basis: hazard identification and analysis via FTA, ETA, FMEA, Cause-consequence Analysis (CCA), etc.
- Many specific techniques
- Related to QA and SRE
- Risk resolution: damage reduction too

---

## Hazard Elimination

### Elimination of hazard
- Intrinsically safe (sub-)system
- All eliminated: Feasibility & cost?
- Certain types of hazard eliminated
- Direct use of hazard identification and analysis results

### Specific techniques: "Good SE & SSE"
- Component substitution
- Fault Tree Analysis
- No single point of failure
- Event Tree Analysis
- Simplification of building blocks
- Decoupling of system architecture
- Human errors/hazardous material elimination

### Component safety certification
- Link to testing/QA activities and software safety program (SSP) process

---

## Hazard Elimination (Continued)

### FTA derived
- Negating/altering logical conditions
- Critical component: Substitution
- Structural change?

### ETA derived
- Altering/forcing event sequences
- No single point of failure
- Rollback of feature possibility?

### FMEA/etc. derived
- Environmental control/influence
- Formal verification of component and/or system
- System Theoretic Accident Mode (STAMP) (Model 4)

---

## Hazard, Controllability, & Observability

Related to hazard resolution, particularly hazard reduction and control.

### Controllability
- Between any two system states
- Desirable/safe states: maintain
- Fail â‡’ action â‡’ safe (hazard control)
- **Controllability limits:**
  - System design/structure limit
  - Energy/capacity limit

### Observability
- Observation of system states (and failures)
- Basis for control

---

## Design for Controllability

### Maintain safe states
- Use built-in control
- Monitoring: observation implies control
- Multiple checks through monitoring
- Mostly in hazard reduction

### Enhancing control opportunities
- Incremental control: More control points
- Intermediate states: More observational points
  - Gives more control opportunities
- Decision aid: Easier/more control points
- Both in hazard reduction and especially in hazard control

---

## Hazard Reduction: Techniques

### Monitoring and checks

- Hardware checks: lowest level
- Code-level checks: assertions
- Audit checks: independent monitoring
- Supervisory checks: system/highest level

---

## Hazard Reduction: Techniques (Continued)

### Locks and barriers (passive)

- Lock-outs (preventing hazard)
- Lock-ins (maintaining safety conditions)
- Interlocks (correct order/combinations)
- Other barriers (extra capital/redundancy/etc.)

---

## Hazard Reduction: Techniques (Further)

---

## Hazard Resolution: SW Interlock

### Software used as safety interlock
- Software usage: Data/control/safety
- Example: Emergency shut-down software

### More stringent safety requirement
- Most software function safety-related
- Should not rely solely on software

### Therac-25 accident lessons
- Series of 1985â€“1987 radiation therapy accidents
- Massive overdoses of radiation to at least six patients
- Race Condition â€“ Tech typed commands too quickly, safety mechanism bypassed
- Lacking hardware interlocks

---

## Hazard Resolution: HW Interlock

### Hardware/software interlock

**Limitation of s/w backups:**
- Diversity and independence problems

**Hardware backups and interlocks:**
- Different characteristics
- Different failure mechanisms
- More likely to be independent

**Passive/active safety devices**
- Combine the advantages implying safety increases

---

## Hazard Control

### Hazard control
- Detecting hazard, then control it
- Built-in control: by design
- **Change after detection:**
  - Passive limits, mostly outside the system
  - Active through control devices and sub-systems

### Specific techniques
- Limiting exposure; duration is reduced
- Isolation and containment
- Protection systems
- Fail-safe design

---

## Hazard Control: Techniques

### Internal system change
- Isolation of hazard event
- Containment around hazard event
- Fail-safe design, a passive approach

### System augmentation
- **Protection system (PS) added on:**
  - Hazard â‡’ PS action â‡’ safe
  - Shut-down or partial shut-down
  - Example; automatic coolant injection or pressure relief
- Controllability limit, done earlier
- **Partial solution may be necessary:**
  - Reduce the severity
  - Bring to a neighboring state

---

## Risk Resolution: Damage Reduction

---